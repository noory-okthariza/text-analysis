---
title: "A Guide to Unsupervised Topic Modelling"
format: html
editor: visual
---

I use the LDA (Latent Dirichlet Allocation) algorithm to conduct an unsupervised topic modeling on the State of the Union addresses delivered by US Presidents from the Democratic Party, starting from President Clinton to the Biden era. I scraped the speech data from [millercenter.org](https://millercenter.org/the-presidency/presidential-speeches) and combined all the speeches into a single document

LDA treats a document as consisting of various topics, and topics are broken down into words. In an unsupervised model, we have no prior knowledge about the document, nor do we classify the document into predefined topics. Since the document consists of only one column and one row, we don't have any covariates/IVs that can be used to make predictions (supervised LDA would be fit for that purpose).

The data is available [here](https://github.com/noory-okthariza/text-analysis).

```{r}
#| message: FALSE
#| warning: FALSE

# Load libraries and data
library(readtext)
library(tidyverse)
library(tidytext)
library(topicmodels)
library(ggplot2)

setwd("~/Desktop/text-analysis/State of the union")
democrat <- readtext("democrat.docx")

```

# Pre-processing the data

Since we don’t have multiple documents, LDA would not have enough information to ‘train’ the data into meaningful outputs. However, we can address this by treating each paragraph as a ‘document’ and letting R analyze patterns from this collection of paragraphs. That way, the multiple documents problem is solved. In my case, I have 2015 ‘documents-paragraphs.’

```{r}
# Clean and split text into paragraphs
democrat_paragraphs <- democrat %>%
  unnest_tokens(paragraph, text, token = "regex", pattern = "\n") %>%
  filter(paragraph != "")


# Create a Document-Term Matrix (DTM) and split paragraphs into words
dtm <- democrat_paragraphs %>%
  unnest_tokens(word, paragraph) %>% # automatically set text to lower cases and remove punctuation
  anti_join(stop_words, by = "word") %>% 
  count(row_number(), word) %>% 
  cast_dtm(document = row_number(), term = word, value = n)
```

# Fitting the model

We fit the model by setting the K-parameter. In this case, it is set to 5. This will allow R to create 5 latent topics from the corpus. Don't forget to always fine-tune the parameter according to your document to make a more meaningful interpretation.

```{r}
# Fit LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))

# Get the top words for each topic
top_words <- terms(lda_model, 10)
```

## 1. Extracting latent topic

Again, we assume no prior knowledge about the topics - the word 'latent' in LDA speak for itself.

```{r}
# Extract the word-topic probabilities and plot top words
tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ggplot(aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(
    title = "Fig 1. Top words for each latent topic",
    x = "Words and Topics",
    y = "Probability"
  ) +
  theme_minimal()
```

## 2. Knowing the most dominant topic

The first analysis gives us the relationship between words and latent topics, but it doesn't tell us which topic is the most dominant in the document. We can obtain this information by using the following algorithm.

```{r}
# Find the dominant topic for each paragraph
doc_topics <- posterior(lda_model)$topics
dominant_topic <- apply(doc_topics, 1, which.max)

# Count the number of paragraphs associated with each topic
topic_counts <- table(dominant_topic)

# Convert the table to a data frame
topic_counts_df <- as.data.frame(topic_counts)
colnames(topic_counts_df) <- c("Topic", "Frequency")

```

```{r}
# Visualize the topic dominance
topic_counts_df %>%
  ggplot(aes(x = as.factor(Topic), y = Frequency, fill = as.factor(Topic))) + 
  geom_bar(stat = "identity", show.legend = F) +
  labs(
    title = "Fig 2. Ranking of dominant topics across documents",
    x = "Topic",
    y = "Number of Documents"
  ) +
  theme_minimal()

```

## 3. Topic co-occurance

Next, we also want to know whether the 5 topics are related to each other—that is, to what extent they are in conversation or stand-alone documents.

```{r}
#| message: false
library(reshape2)

# Calculate pairwise topic co-occurrence probabilities
co_occurrence <- cor(doc_topics)

# Visualize the co-occurrence matrix
melt(co_occurrence) %>%
  ggplot(aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  labs(
    title = "Fig 3. Topic co-occurrence matrix",
    x = "Topic",
    y = "Topic",
    fill = "Correlation"
  ) +
  theme_minimal()
```

## 4. Strength of association between topics and documents

We also want to know how the probability of each topic varies across a corpus of documents. Unlike the second analysis, this analysis focuses on the likelihood of topics throughout the entire corpus rather than calculating individual documents one by one.

```{r}
# Calculate maximum association score for each paragraph
max_topic_association <- apply(doc_topics, 1, max)

# Add document-level information
association_df <- data.frame(
  doc_id = 1:nrow(doc_topics),
  dominant_topic = dominant_topic,
  max_association = max_topic_association
)
```

```{r}
# Visualize the association strength
association_df %>%
  ggplot(aes(x = factor(dominant_topic), y = max_association, fill = factor(dominant_topic))) +
  geom_boxplot(show.legend = F) +
  labs(
    title = "Fig 4. Strength of association between 5 topics and documents",
    x = "Dominant Topic",
    y = "Association Score"
  ) +
  theme_minimal()
```

I think LDA is best suited if your research is still in the exploratory stage and you need additional tools to generate hypotheses or identify patterns from messy, unstructured data or text. It should also work well if you're interested in discourse analysis.

Feel free to reach out if you have any questions!
